This projects deals with the study of the intrinsic dimension of objective landscapes through constraints applied in the space of network parameters. The investigation regards how many degrees of freedom (network parameters) are strictly necessary to solve a certain task almost as good as the standard unconstrained model.

The code for this project is based on the notebook "Autograd and Modules" from the course of "Deep Learning and Applied Artificial Intelligence" held by Prof. Rodol√†:

https://colab.research.google.com/github/erodola/DLAI-s2-2022/blob/main/labs/05/5_Autograd_and_Modules_2022.ipynb#scrollTo=VmFJ7hRMMFRL

As in the notebook the selected task is the classification of hand written digits contained in the MNIST, by means of a MLP.


The papers on which this project is based are the following:

-"MEASURING THE INTRINSIC DIMENSION OF OBJECTIVE LANDSCAPES" by Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski
https://arxiv.org/abs/1804.08838

-"Low Dimensional Landscape Hypothesis is True: DNNs can be Trained in Tiny Subspaces" by Tao Li,Lei Tan, Qinghua Tao, Yipeng Liu, Xiaolin Huang
https://arxiv.org/abs/2103.11154#:~:text=version%2C%20v2)%5D-,Low%20Dimensional%20Landscape%20Hypothesis%20is%20True%3A%20DNNs,be%20Trained%20in%20Tiny%20Subspaces&text=Deep%20neural%20networks%20(DNNs)%20usually,trained%20in%20low%2Ddimensional%20subspaces.
